{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import  tqdm_notebook as tqdm\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.1: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A10. Max memory: 21.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"<path_to_phone_model>\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "    token = \"<hf_token>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages(sentence, examples):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in correcting typos in sentences.\"},\n",
    "        {\"role\": \"user\", \"content\": \"\"\"\n",
    "Here are examples of sentences with typos; learn from them:\n",
    "\n",
    "{examples}\n",
    "Now, please correct this sentence and output only the corrected version with no additional text:\n",
    "\n",
    "{target_sentence}\n",
    "        \"\"\".format(target_sentence=sentence, examples=examples)},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_and_wrong_syllables(true_sentence, predicted_sentence):\n",
    "    # Character-level accuracy using SequenceMatcher\n",
    "    char_matcher = difflib.SequenceMatcher(None, true_sentence, predicted_sentence)\n",
    "    accuracy = char_matcher.ratio()\n",
    "    \n",
    "    # Word-level wrong syllable count using SequenceMatcher on word lists\n",
    "    true_words = true_sentence.split()\n",
    "    predicted_words = predicted_sentence.split()\n",
    "    word_matcher = difflib.SequenceMatcher(None, true_words, predicted_words)\n",
    "    \n",
    "    # Calculate wrong syllables based on insert, delete, and replace operations\n",
    "    wrong_syllables = sum(1 for tag, _, _, _, _ in word_matcher.get_opcodes() if tag in ('insert', 'delete', 'replace'))\n",
    "    \n",
    "    return accuracy, wrong_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_sentence(sentence, examples):\n",
    "    messages = get_messages(sentence, examples)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "    ret = tokenizer.batch_decode(outputs)\n",
    "    gpt_response = re.search(r\"assistant<\\|end_header_id\\|>\\n\\n(.*?)<\\|eot_id\\|>\", ret[0], re.DOTALL)\n",
    "    if gpt_response:\n",
    "        gpt_response = gpt_response.group(1)\n",
    "        return gpt_response\n",
    "    else:\n",
    "        raise ValueError(\"LLM response not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_postprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    # remove all non a-z0-9 \n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_691737/2679182152.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, row in tqdm(df.iterrows(), total=total):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a891bfc023c4bd89b82ab2ca1706c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM Auto] Index: 0 of 1000\n",
      "[LLM Auto] CoAtNet 0.9326923076923077 4\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 100 of 1000\n",
      "[LLM Auto] CoAtNet 0.9452054794520548 3\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 200 of 1000\n",
      "[LLM Auto] CoAtNet 0.9523809523809523 1\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 300 of 1000\n",
      "[LLM Auto] CoAtNet 0.972972972972973 2\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 400 of 1000\n",
      "[LLM Auto] CoAtNet 1.0 0\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 500 of 1000\n",
      "[LLM Auto] CoAtNet 0.8888888888888888 4\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 600 of 1000\n",
      "[LLM Auto] CoAtNet 0.9743589743589743 2\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 700 of 1000\n",
      "[LLM Auto] CoAtNet 0.9574468085106383 3\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 800 of 1000\n",
      "[LLM Auto] CoAtNet 0.9418604651162791 2\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 900 of 1000\n",
      "[LLM Auto] CoAtNet 0.9444444444444444 2\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Model: EchoCrypt\n",
      "[LLM Auto] NF noise_0.012\n",
      "[LLM Auto] LLM Average Accuracy: 0.9974876517465237\n",
      "[LLM Auto] LLM Sum of Wrong Syllables: 112\n",
      "[LLM Auto] ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_691737/2679182152.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, row in tqdm(df.iterrows(), total=total):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcafa3b6e2a4cd5a409bd1e6202370d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM Auto] Index: 0 of 1000\n",
      "[LLM Auto] CoAtNet 0.8269230769230769 4\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 100 of 1000\n",
      "[LLM Auto] CoAtNet 0.8356164383561644 7\n",
      "[LLM Auto] LLM 0.958904109589041 1\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 200 of 1000\n",
      "[LLM Auto] CoAtNet 0.8809523809523809 4\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 300 of 1000\n",
      "[LLM Auto] CoAtNet 0.9054054054054054 4\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 400 of 1000\n",
      "[LLM Auto] CoAtNet 0.8333333333333334 5\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 500 of 1000\n",
      "[LLM Auto] CoAtNet 0.873015873015873 3\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 600 of 1000\n",
      "[LLM Auto] CoAtNet 0.8589743589743589 3\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 700 of 1000\n",
      "[LLM Auto] CoAtNet 0.8829787234042553 3\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 800 of 1000\n",
      "[LLM Auto] CoAtNet 0.872093023255814 4\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 900 of 1000\n",
      "[LLM Auto] CoAtNet 0.8555555555555555 3\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Model: EchoCrypt\n",
      "[LLM Auto] NF noise_0.024\n",
      "[LLM Auto] LLM Average Accuracy: 0.9851784080264059\n",
      "[LLM Auto] LLM Sum of Wrong Syllables: 427\n",
      "[LLM Auto] ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_691737/2679182152.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, row in tqdm(df.iterrows(), total=total):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572855ef9c59435ea4c7264f9c18f777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLM Auto] Index: 0 of 1000\n",
      "[LLM Auto] CoAtNet 0.6634615384615384 3\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 100 of 1000\n",
      "[LLM Auto] CoAtNet 0.6712328767123288 2\n",
      "[LLM Auto] LLM 0.9863013698630136 1\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 200 of 1000\n",
      "[LLM Auto] CoAtNet 0.6547619047619048 2\n",
      "[LLM Auto] LLM 0.8941176470588236 2\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 300 of 1000\n",
      "[LLM Auto] CoAtNet 0.6351351351351351 3\n",
      "[LLM Auto] LLM 0.6575342465753424 2\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 400 of 1000\n",
      "[LLM Auto] CoAtNet 0.6805555555555556 2\n",
      "[LLM Auto] LLM 0.9166666666666666 1\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 500 of 1000\n",
      "[LLM Auto] CoAtNet 0.6825396825396826 3\n",
      "[LLM Auto] LLM 0.9047619047619048 3\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 600 of 1000\n",
      "[LLM Auto] CoAtNet 0.6666666666666666 1\n",
      "[LLM Auto] LLM 0.9316770186335404 1\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 700 of 1000\n",
      "[LLM Auto] CoAtNet 0.7127659574468085 2\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 800 of 1000\n",
      "[LLM Auto] CoAtNet 0.6976744186046512 3\n",
      "[LLM Auto] LLM 0.9265536723163842 2\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Index: 900 of 1000\n",
      "[LLM Auto] CoAtNet 0.6777777777777778 2\n",
      "[LLM Auto] LLM 1.0 0\n",
      "[LLM Auto] ==========\n",
      "[LLM Auto] Model: EchoCrypt\n",
      "[LLM Auto] NF noise_0.06\n",
      "[LLM Auto] LLM Average Accuracy: 0.8952173049358706\n",
      "[LLM Auto] LLM Sum of Wrong Syllables: 1434\n",
      "[LLM Auto] ===\n"
     ]
    }
   ],
   "source": [
    "NFs = [\n",
    "    \"noise_0.012\",\n",
    "    \"noise_0.024\",\n",
    "    \"noise_0.06\",\n",
    "]\n",
    "output_dir = \"echocrypt\"\n",
    "\n",
    "for nf in NFs:\n",
    "    df = pd.read_csv(f\"results/{nf}.csv\")\n",
    "    examples = \"\"\n",
    "\n",
    "    for i in range(2):\n",
    "        examples += f\"\\tsentence: {df['Predicted Sentence'][i]}\\n\"\n",
    "        examples += f\"\\tcorrected: {df['True Sentence'][i]}\\n\\n\"\n",
    "\n",
    "    llm_accs = []\n",
    "    llm_ws = []\n",
    "    llm_sen = []\n",
    "    total=len(df)\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=total):\n",
    "        should_print = index % 100 == 0\n",
    "        predicted_sentence = row['Predicted Sentence']\n",
    "        true_sentence = row['True Sentence']\n",
    "        accuracy, wrong_syllables = compute_accuracy_and_wrong_syllables(true_sentence, predicted_sentence)\n",
    "        if should_print:\n",
    "            print(f\"[LLM Auto] Index: {index} of {total}\")\n",
    "            print(\"[LLM Auto] CoAtNet\", accuracy, wrong_syllables)\n",
    "        \n",
    "        llm_sentence = get_llm_sentence(predicted_sentence, examples)\n",
    "        llm_sentence = llm_postprocess(llm_sentence)\n",
    "        accuracy, wrong_syllables = compute_accuracy_and_wrong_syllables(true_sentence, llm_sentence)\n",
    "        if should_print:\n",
    "            print(\"[LLM Auto] LLM\", accuracy, wrong_syllables)\n",
    "            print(\"[LLM Auto] ==========\")\n",
    "        \n",
    "        llm_sen.append(llm_sentence)\n",
    "        llm_accs.append(accuracy)\n",
    "        llm_ws.append(wrong_syllables)\n",
    "\n",
    "    df['LLM Sentence'] = llm_sen\n",
    "    df['LLM Accuracy'] = llm_accs\n",
    "    df['LLM Wrong syllables'] = llm_ws\n",
    "\n",
    "    # average accuracy\n",
    "    llm_avg_accuracy = sum(llm_accs) / len(llm_accs)\n",
    "    # sum of wrong syllables\n",
    "    llm_sum_wrong_syllables = sum(llm_ws)\n",
    "\n",
    "    print(f\"[LLM Auto] Model: EchoCrypt\")\n",
    "    print(f\"[LLM Auto] NF {nf}\")\n",
    "    print(f\"[LLM Auto] LLM Average Accuracy: {llm_avg_accuracy}\")\n",
    "    print(f\"[LLM Auto] LLM Sum of Wrong Syllables: {llm_sum_wrong_syllables}\")\n",
    "    print(\"[LLM Auto] ===\")\n",
    "    \n",
    "    df.to_csv(f'results/{output_dir}/{nf}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
