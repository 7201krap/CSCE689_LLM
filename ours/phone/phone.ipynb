{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a72dcb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 9.146832,
     "end_time": "2024-07-02T11:21:47.187795",
     "exception": false,
     "start_time": "2024-07-02T11:21:38.040963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import torchaudio\n",
    "\n",
    "from torchaudio.transforms import TimeMasking, FrequencyMasking\n",
    "from torchvision.ops import SqueezeExcitation\n",
    "from torchinfo import summary\n",
    "from torchsummary import summary\n",
    "from torchvision.ops import SqueezeExcitation\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy.io import wavfile\n",
    "from functools import reduce\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fa28a",
   "metadata": {
    "papermill": {
     "duration": 0.01173,
     "end_time": "2024-07-02T11:21:47.212102",
     "exception": false,
     "start_time": "2024-07-02T11:21:47.200372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58baf30f",
   "metadata": {
    "papermill": {
     "duration": 0.02495,
     "end_time": "2024-07-02T11:21:47.249037",
     "exception": false,
     "start_time": "2024-07-02T11:21:47.224087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.n_samples = 0\n",
    "        self.dataset = []\n",
    "        self.labels = set()  # To track unique labels\n",
    "        self.load_audio_files(self.data_dir)\n",
    "\n",
    "    def load_audio_files(self, path: str):\n",
    "        for dirname, _, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirname, filename)\n",
    "                \n",
    "                # label = dirname.split('/')[-1]  # on MAC \n",
    "                label = os.path.basename(dirname)   # on Windows\n",
    "       \n",
    "                # my implementation start\n",
    "                if '0' <= label <= '9':\n",
    "                    label_index = ord(label) - ord('0')\n",
    "                    # print(label_index)\n",
    "                elif 'a' <= label <= 'z':\n",
    "                    label_index = ord(label) - ord('a') + 10\n",
    "                    # print(label_index)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected label: {label}\")\n",
    "                    break\n",
    "                label_tensor = torch.tensor(label_index)\n",
    "                # my implementation done\n",
    "                \n",
    "                # Add the label to the set of unique labels\n",
    "                self.labels.add(label_tensor.item())\n",
    "                \n",
    "                # Load audio\n",
    "                waveform, sample_rate = torchaudio.load(file_path)\n",
    "                if self.transform is not None:\n",
    "                    waveform_transformed = self.transform(waveform)\n",
    "                \n",
    "                if waveform_transformed.shape[2] != 64:\n",
    "                    print(\"Wrong shape:\", waveform_transformed.shape)\n",
    "                    continue\n",
    "                \n",
    "                self.n_samples += 1\n",
    "                self.dataset.append((waveform, label_tensor))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, label = self.dataset[idx]\n",
    "        return waveform, label\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.labels)  # Return the number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9548a87",
   "metadata": {
    "papermill": {
     "duration": 23.043196,
     "end_time": "2024-07-02T11:22:10.304055",
     "exception": false,
     "start_time": "2024-07-02T11:21:47.260859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "to_mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate, n_mels=64, hop_length=300, n_fft=2048, win_length=1024)\n",
    "mel_spectrogram_to_numpy = lambda spectrogram: spectrogram.log2()[0,:,:].numpy()\n",
    "transforms = Compose([to_mel_spectrogram, mel_spectrogram_to_numpy, ToTensor()])\n",
    "dataset = AudioDataset('../../new_dataset_phone', transforms)\n",
    "print(\"number of classes:\", dataset.num_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ffeca",
   "metadata": {
    "papermill": {
     "duration": 0.022289,
     "end_time": "2024-07-02T11:22:10.339222",
     "exception": false,
     "start_time": "2024-07-02T11:22:10.316933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Assume your dataset has a 'targets' attribute or you can extract labels from it\n",
    "targets = [data[1] for data in dataset]  # Assuming dataset returns (data, label) pairs\n",
    "\n",
    "# Split the dataset indices with stratification\n",
    "train_indices, tmp_indices = train_test_split(\n",
    "    range(len(dataset)), \n",
    "    test_size=0.3,  # 30% of the data goes to val+test\n",
    "    stratify=targets\n",
    ")\n",
    "\n",
    "val_indices, test_indices = train_test_split(\n",
    "    tmp_indices, \n",
    "    test_size=0.33,  # 33% of the 30% goes to the test set, i.e., 10% of the original dataset\n",
    "    stratify=[targets[i] for i in tmp_indices]\n",
    ")\n",
    "\n",
    "# Create subsets of the dataset based on the indices\n",
    "init_train_set = torch.utils.data.Subset(dataset, train_indices)\n",
    "init_val_set = torch.utils.data.Subset(dataset, val_indices)\n",
    "init_test_set = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "# Print the sizes for verification\n",
    "print(\"Sample rate:\", sample_rate)\n",
    "print(f\"Train set size: {len(init_train_set)}, Validation set size: {len(init_val_set)}, Test set size: {len(init_test_set)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb620f7",
   "metadata": {
    "papermill": {
     "duration": 0.011869,
     "end_time": "2024-07-02T11:22:10.363191",
     "exception": false,
     "start_time": "2024-07-02T11:22:10.351322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data augmentation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896672e",
   "metadata": {
    "papermill": {
     "duration": 0.020245,
     "end_time": "2024-07-02T11:22:10.395502",
     "exception": false,
     "start_time": "2024-07-02T11:22:10.375257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, base_dataset, transformations):\n",
    "        super(TrainingDataset, self).__init__()\n",
    "        self.base = base_dataset\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, label = self.base[idx]\n",
    "        return self.transformations(waveform), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad77d60",
   "metadata": {
    "papermill": {
     "duration": 0.020136,
     "end_time": "2024-07-02T11:22:10.427648",
     "exception": false,
     "start_time": "2024-07-02T11:22:10.407512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeShifting():\n",
    "    def __call__(self, samples):\n",
    "        samples = samples.numpy()        \n",
    "        shift = int(samples.shape[1] * 0.3)\n",
    "        random_shift = random.randint(0, shift)\n",
    "        data_roll = np.zeros_like(samples)\n",
    "        data_roll[0] = np.roll(samples[0], random_shift)\n",
    "        data_roll[1] = np.roll(samples[1], random_shift)\n",
    "        return torch.tensor(data_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81daa2",
   "metadata": {
    "papermill": {
     "duration": 0.02096,
     "end_time": "2024-07-02T11:22:10.461023",
     "exception": false,
     "start_time": "2024-07-02T11:22:10.440063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_transforms = Compose([\n",
    "    TimeShifting(),\n",
    "    to_mel_spectrogram, mel_spectrogram_to_numpy, ToTensor(),\n",
    "    FrequencyMasking(7),\n",
    "    TimeMasking(7),\n",
    "    FrequencyMasking(7),\n",
    "    TimeMasking(7)\n",
    "])\n",
    "\n",
    "train_set = TrainingDataset(init_train_set, aug_transforms)\n",
    "train_set_no_aug = TrainingDataset(init_train_set, transforms)\n",
    "val_set = TrainingDataset(init_val_set, transforms)\n",
    "test_set = TrainingDataset(init_test_set, transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de10ccb",
   "metadata": {
    "papermill": {
     "duration": 0.012192,
     "end_time": "2024-07-02T11:22:10.485395",
     "exception": false,
     "start_time": "2024-07-02T11:22:10.473203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training set image (augmented vs non-augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1a69a",
   "metadata": {
    "papermill": {
     "duration": 0.251803,
     "end_time": "2024-07-02T11:22:10.749292",
     "exception": false,
     "start_time": "2024-07-02T11:22:10.497489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.imshow(train_set[0][0][0], cmap='viridis')\n",
    "ax2.imshow(val_set[0][0][0], cmap='viridis')\n",
    "print(train_set[0][0][0].shape)\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of classes:\", dataset.num_classes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca1c64",
   "metadata": {
    "papermill": {
     "duration": 0.013003,
     "end_time": "2024-07-02T11:22:11.086758",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.073755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CoAtNet (Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff42682",
   "metadata": {
    "papermill": {
     "duration": 0.02146,
     "end_time": "2024-07-02T11:22:11.121284",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.099824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(1, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e26138",
   "metadata": {
    "papermill": {
     "duration": 0.023276,
     "end_time": "2024-07-02T11:22:11.157751",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.134475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor=4):\n",
    "        super().__init__()\n",
    "        self.mb_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, in_channels * expansion_factor, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels * expansion_factor),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels * expansion_factor, in_channels * expansion_factor, kernel_size=3, padding=1, groups=in_channels * expansion_factor),\n",
    "            nn.BatchNorm2d(in_channels * expansion_factor),\n",
    "            nn.GELU(),\n",
    "            SqueezeExcitation(in_channels * expansion_factor, in_channels, activation=nn.GELU),\n",
    "            nn.Conv2d(in_channels * expansion_factor, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.mb_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94b4ea",
   "metadata": {
    "papermill": {
     "duration": 0.022034,
     "end_time": "2024-07-02T11:22:11.193072",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.171038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownsamplingMBConv(MBConv):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor=4):\n",
    "        super().__init__(in_channels, out_channels, expansion_factor=4)\n",
    "        self.mb_conv[1] = nn.Conv2d(in_channels, in_channels * expansion_factor, kernel_size=1, stride = 2)\n",
    "        self.channel_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.channel_projection(self.pool(x)) + self.mb_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85154733",
   "metadata": {
    "papermill": {
     "duration": 0.034533,
     "end_time": "2024-07-02T11:22:11.240578",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.206045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RelativeAttention2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size, heads=8, head_size=32):\n",
    "        super().__init__()\n",
    "        heads = out_channels // head_size\n",
    "        self.heads = heads\n",
    "        self.head_size = head_size\n",
    "        self.image_size = image_size\n",
    "        self.head_dim = heads * head_size\n",
    "        self.attend = nn.Softmax(dim=-2) # Taken from My_CoAtNet\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.to_q = nn.Linear(in_channels, self.head_dim)\n",
    "        self.to_k = nn.Linear(in_channels, self.head_dim)\n",
    "        self.to_v = nn.Linear(in_channels, self.head_dim)\n",
    "        self.to_output = nn.Sequential(\n",
    "            nn.Linear(self.head_dim, out_channels),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.normalization = nn.LayerNorm(in_channels)\n",
    "        \n",
    "        self.relative_bias = nn.Parameter(torch.randn(heads, (2 * image_size - 1) * (2 * image_size - 1)))\n",
    "        self.register_buffer(\"relative_indices\", self.get_indices(image_size, image_size)) # Taken from My_CoAtNet\n",
    "        self.precomputed_relative_bias = None\n",
    "    \n",
    "    def norm(self, x):\n",
    "        x = x.transpose(1, -1) # Taken from My_CoAtNet\n",
    "        x = self.normalization(x)\n",
    "        x = x.transpose(-1, 1) # Taken from My_CoAtNet\n",
    "        return x\n",
    "    \n",
    "    def get_relative_biases(self):\n",
    "        # Relative bias caching mentioned in CoAtNet: Marrying Convolution and Attention for All Data Sizes\n",
    "        if not self.training:\n",
    "            return self.precomputed_relative_bias\n",
    "        # Taken from od My_CoAtNet\n",
    "        indices = self.relative_indices.expand(self.heads, -1)\n",
    "        rel_pos_enc = self.relative_bias.gather(-1, indices)\n",
    "        rel_pos_enc = rel_pos_enc.unflatten(-1, (self.image_size * self.image_size, self.image_size * self.image_size))\n",
    "        return rel_pos_enc\n",
    "    \n",
    "    def reshape_for_linear(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        return x.reshape(b, self.image_size * self.image_size, self.in_channels)\n",
    "    \n",
    "    def attention_score(self, x):\n",
    "        b, _, h, _ = x.shape\n",
    "        q = self.to_q(self.reshape_for_linear(x)).view(b, self.heads, self.head_size, -1) # Taken from My_CoAtNet\n",
    "        k = self.to_k(self.reshape_for_linear(x)).view(b, self.heads, self.head_size, -1) # Taken from My_CoAtNet\n",
    "        dots = torch.matmul(k.transpose(-1, -2), q) / math.sqrt(self.head_dim)\n",
    "        relative_biases_indexed = self.get_relative_biases()\n",
    "        return self.attend(dots + relative_biases_indexed)\n",
    "    \n",
    "    def relative_attention(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        v = self.to_v(self.reshape_for_linear(x)).view(b, self.heads, self.head_size, -1) # Taken from My_CoAtNet\n",
    "        out = torch.matmul(v, self.attention_score(x)) # I figured this out after debugging (Still the same as My_CoAtNet)\n",
    "        out = out.view(b, self.image_size, self.image_size, -1)\n",
    "        return self.to_output(out).view(b, self.out_channels, self.image_size, self.image_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.relative_attention(self.norm(x))\n",
    "    \n",
    "    def train(self, training):\n",
    "        if not training:\n",
    "            self.precomputed_relative_bias = self.get_relative_biases()\n",
    "        super().train(training)\n",
    "        \n",
    "    # Taken from My_CoAtNet\n",
    "    @staticmethod\n",
    "    def get_indices(h, w):\n",
    "        y = torch.arange(h, dtype=torch.long)\n",
    "        x = torch.arange(w, dtype=torch.long)\n",
    "        \n",
    "        y1, x1, y2, x2 = torch.meshgrid(y, x, y, x)\n",
    "        indices = (y1 - y2 + h - 1) * (2 * w - 1) + x1 - x2 + w - 1\n",
    "        indices = indices.flatten()\n",
    "        \n",
    "        return indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909ce75",
   "metadata": {
    "papermill": {
     "duration": 0.022918,
     "end_time": "2024-07-02T11:22:11.276610",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.253692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownsamplingRelativeAttention2d(RelativeAttention2d):\n",
    "    def __init__(self, in_channels, out_channels, image_size, heads=8, head_size=32):\n",
    "        super().__init__(in_channels, out_channels, image_size, heads=8, head_size=32)\n",
    "        self.channel_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.normalization = nn.LayerNorm(in_channels)\n",
    "\n",
    "\n",
    "    def norm(self, x):\n",
    "        x = x.transpose(1, -1) # Taken from My_CoAtNet\n",
    "        x = self.normalization(x)\n",
    "        x = x.transpose(-1, 1) # Taken from My_CoAtNet\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.channel_projection(self.pool(x)) + self.relative_attention(self.pool(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7c647",
   "metadata": {
    "papermill": {
     "duration": 0.023138,
     "end_time": "2024-07-02T11:22:11.312722",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.289584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, out_channels, expansion_factor=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = out_channels * expansion_factor\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, out_channels),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.normalization = nn.LayerNorm(out_channels)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    \n",
    "    def norm(self, x):\n",
    "        x = x.transpose(1, -1) # Taken from My_CoAtNet\n",
    "        x = self.normalization(x)\n",
    "        x = x.transpose(-1, 1) # Taken from My_CoAtNet\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        old_shape = x.shape\n",
    "        batch_size = old_shape[0]\n",
    "        return x + torch.reshape(self.ffn(torch.reshape(self.norm(x), (batch_size, -1, self.out_channels))), old_shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6c16f",
   "metadata": {
    "papermill": {
     "duration": 0.020682,
     "end_time": "2024-07-02T11:22:11.346783",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.326101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownsampleTransformerBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, image_size):\n",
    "        attention = DownsamplingRelativeAttention2d(in_channels, out_channels, image_size)\n",
    "        ffn = FeedForwardNetwork(out_channels)\n",
    "        super().__init__(\n",
    "            attention,\n",
    "            ffn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aef4f2",
   "metadata": {
    "papermill": {
     "duration": 0.02029,
     "end_time": "2024-07-02T11:22:11.379982",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.359692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, image_size):\n",
    "        attention = RelativeAttention2d(in_channels, out_channels, image_size)\n",
    "        ffn = FeedForwardNetwork(out_channels)\n",
    "        super().__init__(\n",
    "            attention,\n",
    "            ffn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabd6da",
   "metadata": {
    "papermill": {
     "duration": 0.022473,
     "end_time": "2024-07-02T11:22:11.457013",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.434540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(in_channels, num_classes)\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.pool(x)\n",
    "        x = torch.reshape(x, (batch_size, -1, self.in_channels))\n",
    "        return torch.squeeze(self.fc(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a61b6",
   "metadata": {
    "papermill": {
     "duration": 0.025704,
     "end_time": "2024-07-02T11:22:11.496233",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.470529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyCoAtNet(nn.Sequential):\n",
    "    def __init__(self, nums_blocks, layer_out_channels, num_classes=36):\n",
    "        s0 = nn.Sequential(Stem(layer_out_channels[0]))\n",
    "        \n",
    "        s1 = [DownsamplingMBConv(layer_out_channels[0], layer_out_channels[1])]\n",
    "        for i in range(nums_blocks[1] - 1):\n",
    "            s1.append(MBConv(layer_out_channels[1], layer_out_channels[1]))\n",
    "        s1 = nn.Sequential(*s1)\n",
    "        \n",
    "        s2 = [DownsamplingMBConv(layer_out_channels[1], layer_out_channels[2])]\n",
    "        for i in range(nums_blocks[2] - 1):\n",
    "            s2.append(MBConv(layer_out_channels[2], layer_out_channels[2]))\n",
    "        s2 = nn.Sequential(*s2)\n",
    "        \n",
    "        s3 = [DownsampleTransformerBlock(layer_out_channels[2], layer_out_channels[3], 64 // 16)]\n",
    "        for i in range(nums_blocks[3] - 1):\n",
    "            s3.append(TransformerBlock(layer_out_channels[3], layer_out_channels[3], 64 // 16))\n",
    "        s3 = nn.Sequential(*s3)\n",
    "        \n",
    "        s4 = [DownsampleTransformerBlock(layer_out_channels[3], layer_out_channels[4], 64 // 32)]\n",
    "        for i in range(nums_blocks[4] - 1):\n",
    "            s4.append(TransformerBlock(layer_out_channels[4], layer_out_channels[4], 64 // 32))\n",
    "        s4 = nn.Sequential(*s4)\n",
    "        \n",
    "        head = Head(layer_out_channels[4], num_classes)\n",
    "        \n",
    "        super().__init__(\n",
    "            s0,\n",
    "            s1,\n",
    "            s2,\n",
    "            s3,\n",
    "            s4,\n",
    "            head\n",
    "        )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03287a",
   "metadata": {
    "papermill": {
     "duration": 0.262523,
     "end_time": "2024-07-02T11:22:11.772171",
     "exception": false,
     "start_time": "2024-07-02T11:22:11.509648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CoAtNet-1\n",
    "nums_blocks = [2, 2, 3, 5, 2]           # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "\n",
    "model = MyCoAtNet(nums_blocks, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f89c5",
   "metadata": {
    "papermill": {
     "duration": 0.022066,
     "end_time": "2024-07-02T11:22:25.137146",
     "exception": false,
     "start_time": "2024-07-02T11:22:25.115080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_linear(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44803714",
   "metadata": {
    "papermill": {
     "duration": 1.366652,
     "end_time": "2024-07-02T11:22:26.517215",
     "exception": false,
     "start_time": "2024-07-02T11:22:25.150563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.apply(init_linear)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))\n",
    "\n",
    "summary(model, input_size=(1, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac96783",
   "metadata": {
    "papermill": {
     "duration": 0.013804,
     "end_time": "2024-07-02T11:22:26.545582",
     "exception": false,
     "start_time": "2024-07-02T11:22:26.531778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb2b27",
   "metadata": {
    "papermill": {
     "duration": 0.02258,
     "end_time": "2024-07-02T11:22:26.581991",
     "exception": false,
     "start_time": "2024-07-02T11:22:26.559411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769341f1",
   "metadata": {
    "papermill": {
     "duration": 0.02428,
     "end_time": "2024-07-02T11:22:26.655313",
     "exception": false,
     "start_time": "2024-07-02T11:22:26.631033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from My_CoAtNet\n",
    "def separate_parameters(model):\n",
    "    parameters_decay = set()\n",
    "    parameters_no_decay = set()\n",
    "    modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
    "    modules_no_weight_decay = (nn.LayerNorm, nn.BatchNorm2d)\n",
    "    \n",
    "    for m_name, m in model.named_modules():\n",
    "        for param_name, param in m.named_parameters():\n",
    "            full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
    "\n",
    "            if isinstance(m, modules_no_weight_decay):\n",
    "                parameters_no_decay.add(full_param_name)\n",
    "            elif param_name.endswith(\"bias\"):\n",
    "                parameters_no_decay.add(full_param_name)\n",
    "            elif isinstance(m, modules_weight_decay):\n",
    "                parameters_decay.add(full_param_name)\n",
    "    \n",
    "    # sanity check\n",
    "    assert len(parameters_decay & parameters_no_decay) == 0\n",
    "    assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
    "\n",
    "    return parameters_decay, parameters_no_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa505577",
   "metadata": {
    "papermill": {
     "duration": 0.031917,
     "end_time": "2024-07-02T11:22:26.701127",
     "exception": false,
     "start_time": "2024-07-02T11:22:26.669210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.1\n",
    "num_epochs = 1100\n",
    "min_learning_rate = 1e-6\n",
    "\n",
    "param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "parameters_decay, parameters_no_decay = separate_parameters(model)\n",
    "parameter_groups = [\n",
    "    {\"params\": [param_dict[pn] for pn in parameters_decay], \"weight_decay\": weight_decay},\n",
    "    {\"params\": [param_dict[pn] for pn in parameters_no_decay], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.Adam(parameter_groups, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=min_learning_rate / learning_rate, total_iters=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d43b9",
   "metadata": {
    "papermill": {
     "duration": 0.022543,
     "end_time": "2024-07-02T11:22:26.737669",
     "exception": false,
     "start_time": "2024-07-02T11:22:26.715126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from My_CoAtNet\n",
    "def plot_results(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"Training loss\", \"Validation loss\"], loc =  \"best\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.plot(train_accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"Training accuracy\", \"Validation accuracy\"], loc =  \"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388eed6c",
   "metadata": {
    "papermill": {
     "duration": 0.020515,
     "end_time": "2024-07-02T11:22:26.772216",
     "exception": false,
     "start_time": "2024-07-02T11:22:26.751701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4614599",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"CoAtNet-1-Best-Phone.pkl\"\n",
    "model_path = \"CoAtNet-1-Phone.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06992ae6",
   "metadata": {
    "papermill": {
     "duration": 2918.17102,
     "end_time": "2024-07-02T12:11:04.957228",
     "exception": false,
     "start_time": "2024-07-02T11:22:26.786208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.squeeze(labels).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = cost(outputs, torch.squeeze(labels))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        previous_lr = scheduler.get_last_lr()[0]\n",
    "        scheduler.step()\n",
    "        next_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {correct/total}\", \"Correct:\", correct, \"Total:\", total, f\"LR: {previous_lr} -> {next_lr}\")\n",
    "        train_losses.append(loss.item())\n",
    "        train_accuracies.append(correct/total)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in val_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = torch.squeeze(labels).to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = cost(outputs, torch.squeeze(labels))\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            print(f\"Validation Accuracy: {correct/total}\", f\"Validation loss: {loss}\", \"Correct:\", correct, \"Total:\", total)\n",
    "            val_losses.append(loss.item())\n",
    "            val_accuracies.append(correct/total)\n",
    "        \n",
    "        if epoch != 0 and (epoch % 50 == 49):\n",
    "            plot_results(train_losses, train_accuracies, val_losses, val_accuracies)\n",
    "        if epoch > 2 and val_accuracies[-1] > max(val_accuracies[:-1]):\n",
    "            save_model(model, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd640e5",
   "metadata": {
    "papermill": {
     "duration": 0.357434,
     "end_time": "2024-07-02T12:11:05.562751",
     "exception": false,
     "start_time": "2024-07-02T12:11:05.205317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5925fc3",
   "metadata": {
    "papermill": {
     "duration": 0.2013,
     "end_time": "2024-07-02T12:11:05.964821",
     "exception": false,
     "start_time": "2024-07-02T12:11:05.763521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32c2da",
   "metadata": {
    "papermill": {
     "duration": 0.32343,
     "end_time": "2024-07-02T12:11:06.488806",
     "exception": false,
     "start_time": "2024-07-02T12:11:06.165376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3b612",
   "metadata": {
    "papermill": {
     "duration": 0.209675,
     "end_time": "2024-07-02T12:11:06.900251",
     "exception": false,
     "start_time": "2024-07-02T12:11:06.690576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Best validation acc: \", max(val_accuracies))\n",
    "print(\"Last validation acc: \", val_accuracies[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271fec36",
   "metadata": {
    "papermill": {
     "duration": 0.211105,
     "end_time": "2024-07-02T12:11:07.721656",
     "exception": false,
     "start_time": "2024-07-02T12:11:07.510551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the alphabet (0-9 and A-Z)\n",
    "digits = [str(digit) for digit in range(10)]\n",
    "alphabet = [chr(ascii_code) for ascii_code in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# Combine digits and alphabet\n",
    "all_characters = digits + alphabet\n",
    "\n",
    "# Create the label dictionary\n",
    "label_dict = {i: all_characters[i] for i in range(len(all_characters))}\n",
    "\n",
    "# Print the label dictionary\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dab350",
   "metadata": {
    "papermill": {
     "duration": 0.231707,
     "end_time": "2024-07-02T12:11:08.158636",
     "exception": false,
     "start_time": "2024-07-02T12:11:07.926929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from My_CoAtNet\n",
    "predictions_path = 'predictions-phone.csv'\n",
    "\n",
    "def predict(data_loader):\n",
    "\n",
    "    original_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for inputs, labels in data_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.squeeze(labels).to(device)\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.shape)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "      \n",
    "        original_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    prediction_df = pd.DataFrame({\"predicted_labels\": [label_dict[pred] for pred in predicted_labels], \"true_labels\": [label_dict[label] for label in original_labels]})\n",
    "    prediction_df.to_csv(predictions_path)\n",
    "\n",
    "    print(f\"Test Accuracy: {correct/total}\")\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    display(prediction_df.T)\n",
    "    \n",
    "    print (metrics.accuracy_score(original_labels, predicted_labels))\n",
    "    print (metrics.classification_report(original_labels, predicted_labels))\n",
    "    \n",
    "    return confusion_matrix(original_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca1eb0",
   "metadata": {
    "papermill": {
     "duration": 0.200093,
     "end_time": "2024-07-02T12:11:08.560637",
     "exception": false,
     "start_time": "2024-07-02T12:11:08.360544",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60e6d2",
   "metadata": {
    "papermill": {
     "duration": 0.849942,
     "end_time": "2024-07-02T12:11:09.609783",
     "exception": false,
     "start_time": "2024-07-02T12:11:08.759841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40241f",
   "metadata": {
    "papermill": {
     "duration": 0.201932,
     "end_time": "2024-07-02T12:11:10.023024",
     "exception": false,
     "start_time": "2024-07-02T12:11:09.821092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae6679",
   "metadata": {
    "papermill": {
     "duration": 2.482162,
     "end_time": "2024-07-02T12:11:12.754519",
     "exception": false,
     "start_time": "2024-07-02T12:11:10.272357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.heatmap(cm,\n",
    "                annot=True,\n",
    "                fmt='g',\n",
    "                xticklabels=all_characters, \n",
    "                yticklabels=all_characters,\n",
    "                annot_kws={\"size\": 12}\n",
    "                )\n",
    "fig.savefig(\"confusion_matrix_CoAtNet-1-phone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741289c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4960306,
     "sourceId": 8349281,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 59598,
     "sourceId": 71358,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2980.75615,
   "end_time": "2024-07-02T12:11:16.111090",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-02T11:21:35.354940",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
