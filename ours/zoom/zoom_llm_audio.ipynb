{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import torchaudio\n",
    "import csv\n",
    "import string\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchaudio.transforms import TimeMasking, FrequencyMasking\n",
    "from torchvision.ops import SqueezeExcitation\n",
    "from torchinfo import summary\n",
    "from torchsummary import summary\n",
    "from torchvision.ops import SqueezeExcitation\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from scipy.io import wavfile\n",
    "from functools import reduce\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_s = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "labels = list(keys_s)\n",
    "keys = [k + '.wav' for k in labels]\n",
    "data_dict = {'Key':[], 'File':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def add_noise_to_stroke(stroke, noise_factor=None):\n",
    "#     # print(stroke.shape)\n",
    "#     noise = torch.randn_like(stroke)\n",
    "#     return stroke + noise_factor * noise\n",
    "\n",
    "def add_noise_to_stroke(stroke, noise_factor=None):\n",
    "    \"\"\"\n",
    "    Adds random noise to an audio stroke tensor.\n",
    "    \n",
    "    Args:\n",
    "    - stroke (torch.Tensor): The input audio stroke tensor.\n",
    "    - noise_factor (float): The factor determining the noise level relative to the stroke's signal.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The noisy audio stroke tensor.\n",
    "    \"\"\"\n",
    "    # Generate random noise\n",
    "    noise = torch.randn(stroke.size()) * noise_factor\n",
    "\n",
    "    # Calculate the standard deviation of the stroke signal\n",
    "    std_dev = stroke.std()\n",
    "\n",
    "    # Scale the noise level based on the signal's standard deviation\n",
    "    noise = noise * std_dev\n",
    "\n",
    "    # Add noise to the original stroke\n",
    "    noisy_stroke = stroke + noise\n",
    "    \n",
    "    return noisy_stroke\n",
    "\n",
    "def isolator(signal, sample_rate, size, scan, before, after, threshold, show=False):\n",
    "    strokes = []\n",
    "    if show:\n",
    "        plt.figure(figsize=(7, 2))\n",
    "        librosa.display.waveshow(signal, sr=sample_rate)\n",
    "    fft = librosa.stft(signal, n_fft=size, hop_length=scan)\n",
    "    energy = np.abs(np.sum(fft, axis=0)).astype(float)\n",
    "    if show:\n",
    "        plt.figure(figsize=(7, 2))\n",
    "        librosa.display.waveshow(energy)\n",
    "    threshed = energy > threshold\n",
    "    if show:\n",
    "        plt.figure(figsize=(7, 2))\n",
    "        librosa.display.waveshow(threshed.astype(float))\n",
    "    peaks = np.where(threshed)[0]\n",
    "    peak_count = len(peaks)\n",
    "    prev_end = sample_rate * 0.1 * (-1)\n",
    "    for i in range(peak_count):\n",
    "        this_peak = peaks[i]\n",
    "        timestamp = (this_peak * scan) + size // 2\n",
    "        if timestamp > prev_end + (0.1 * sample_rate):\n",
    "            keystroke = signal[timestamp - before:timestamp + after]\n",
    "            strokes.append(torch.tensor(keystroke)[None, :])\n",
    "            if show:\n",
    "                plt.figure(figsize=(7, 2))\n",
    "                librosa.display.waveshow(keystroke, sr=sample_rate)\n",
    "            prev_end = timestamp + after\n",
    "    return strokes\n",
    "\n",
    "def convert_to_df_with_noise(audio_dir, noise_factor=None):\n",
    "    \"\"\"\n",
    "    Processes each audio file by isolating strokes, adds noise to each stroke,\n",
    "    and stores the strokes with labels in a dataframe.\n",
    "    \"\"\"\n",
    "    # Generate a list of .wav files in the provided directory\n",
    "    keys = sorted([f for f in os.listdir(audio_dir) if f.endswith('.wav')])\n",
    "    \n",
    "    data_dict = {'Key': [], 'File': []}\n",
    "    \n",
    "    # If your labels are inferred from file names, you can extract them here.\n",
    "    # For example, if the file is 'a.wav', you might want to use 'a' as the label.\n",
    "    def extract_label(filename):\n",
    "        base = os.path.splitext(filename)[0]\n",
    "        return base  # or apply any mapping you need\n",
    "\n",
    "    for file in keys:\n",
    "        loc = os.path.join(audio_dir, file)\n",
    "        samples, sample_rate = librosa.load(loc, sr=None)\n",
    "        strokes = []\n",
    "        prom = 0.06\n",
    "        step = 0.005\n",
    "        # Adjust threshold until we get exactly 25 strokes (or break if not possible)\n",
    "        while not len(strokes) == 25:\n",
    "            strokes = isolator(samples[1 * sample_rate:], sample_rate, 48, 24, 2400, 12000, prom, show=False)\n",
    "            if len(strokes) < 25:\n",
    "                prom -= step\n",
    "            elif len(strokes) > 25:\n",
    "                prom += step\n",
    "            if prom <= 0:\n",
    "                print('-- not possible for:', file)\n",
    "                break\n",
    "            step *= 0.99\n",
    "        \n",
    "        # Apply noise to each extracted stroke\n",
    "        noisy_strokes = [add_noise_to_stroke(stroke, noise_factor) for stroke in strokes]\n",
    "\n",
    "        # Extract a label for this file (modify as needed)\n",
    "        label = extract_label(file)\n",
    "        data_dict['Key'] += [label] * len(noisy_strokes)\n",
    "        data_dict['File'] += noisy_strokes\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    # Optionally, create a mapping for the labels (if needed)\n",
    "    mapper = {}\n",
    "    counter = 0\n",
    "    for l in df['Key']:\n",
    "        if l not in mapper:\n",
    "            mapper[l] = counter\n",
    "            counter += 1\n",
    "    df.replace({'Key': mapper}, inplace=True)\n",
    "    \n",
    "    return df, sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Key                                               File\n",
      "0   0  [[tensor(0.0001), tensor(0.0010), tensor(-0.00...\n",
      "1   0  [[tensor(0.0025), tensor(0.0017), tensor(-0.00...\n",
      "2   0  [[tensor(0.0019), tensor(-0.0025), tensor(0.00...\n",
      "3   0  [[tensor(0.0027), tensor(0.0031), tensor(0.001...\n",
      "4   0  [[tensor(-0.0003), tensor(0.0023), tensor(-0.0...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 900 entries, 0 to 899\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Key     900 non-null    object\n",
      " 1   File    900 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 14.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data_frame, sr = convert_to_df_with_noise(\"../../dataset/Zoom/\", noise_factor=1.0)\n",
    "\n",
    "print(data_frame.head())\n",
    "print(data_frame.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rate: 32000\n",
      "630 180 90\n"
     ]
    }
   ],
   "source": [
    "train_set, tmp_set = train_test_split(data_frame, test_size=0.3, stratify=data_frame['Key'])\n",
    "val_set, test_set = train_test_split(tmp_set, test_size=0.33, stratify=tmp_set['Key'])\n",
    "\n",
    "print(\"Sample rate:\", sr)\n",
    "print(len(train_set), len(val_set), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_name, transform = None, aug = None):\n",
    "        df = file_name\n",
    "        self.transform = transform\n",
    "        self.aug = aug\n",
    "        self.labels = df['Key']\n",
    "        self.values = df['File']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        label = self.labels.iloc[index]\n",
    "        value = self.values.iloc[index]\n",
    "        waveform = self.values.iloc[index]\n",
    "        label = self.labels.iloc[index]\n",
    "        if self.transform:\n",
    "            waveform = waveform.numpy()\n",
    "            waveform = waveform.flatten()\n",
    "            waveform = self.transform(waveform)\n",
    "        if self.aug:\n",
    "            waveform = self.aug(waveform)\n",
    "        return waveform, label\n",
    "\n",
    "class ToMelSpectrogram:\n",
    "    def __call__(self, samples):\n",
    "#         samples = np.array(samples)\n",
    "        spec = librosa.feature.melspectrogram(y = samples, sr = sr, n_mels=64, n_fft=2048, win_length=1024, hop_length=226)\n",
    "        return librosa.power_to_db(spec)\n",
    "\n",
    "class TimeShifting():\n",
    "    def __call__(self, samples):\n",
    "        samples = samples.numpy()\n",
    "        samples = samples.flatten()\n",
    "        \n",
    "        shift = int(len(samples) * 0.4)\n",
    "        random_shift =random.randint(0, shift)\n",
    "        data_roll = np.roll(samples, random_shift)\n",
    "        return data_roll\n",
    "    \n",
    "class SpecAugment():\n",
    "    def __call__(self, samples):\n",
    "        num_mask = 2\n",
    "        freq_masking_max_percentage=0.10\n",
    "        time_masking_max_percentage=0.10\n",
    "        spec = samples.copy()\n",
    "        mean_value = spec.mean()\n",
    "        for i in range(num_mask):\n",
    "            all_frames_num, all_freqs_num = spec.shape[1], spec.shape[1] \n",
    "            freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\n",
    "\n",
    "            num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n",
    "            f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n",
    "            f0 = int(f0)\n",
    "            spec[:, f0:f0 + num_freqs_to_mask] = mean_value\n",
    "\n",
    "            time_percentage = random.uniform(0.0, time_masking_max_percentage)\n",
    "\n",
    "            num_frames_to_mask = int(time_percentage * all_frames_num)\n",
    "            t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n",
    "            t0 = int(t0)\n",
    "            spec[t0:t0 + num_frames_to_mask, :] = mean_value\n",
    "        return spec\n",
    "\n",
    "aug = Compose([\n",
    "    TimeShifting(),\n",
    "    ToMelSpectrogram(),\n",
    "    SpecAugment(),\n",
    "    ToTensor()\n",
    "    ])\n",
    "\n",
    "transform = Compose([\n",
    "    ToMelSpectrogram(),\n",
    "    ToTensor()])\n",
    "        \n",
    "train_set = MyDataset(train_set, aug = aug)\n",
    "val_set = MyDataset(val_set, transform = transform)\n",
    "test_set = MyDataset(test_set, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set image (augmented vs non-augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.imshow(train_set[0][0][0], cmap='viridis')\n",
    "ax2.imshow(val_set[0][0][0], cmap='viridis')\n",
    "print(train_set[0][0][0].shape)\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 36\n"
     ]
    }
   ],
   "source": [
    "print(\"number of classes:\", len(np.unique(data_frame['Key'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoAtNet (Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stem(nn.Sequential):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(1, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor=4):\n",
    "        super().__init__()\n",
    "        self.mb_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.Conv2d(in_channels, in_channels * expansion_factor, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels * expansion_factor),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels * expansion_factor, in_channels * expansion_factor, kernel_size=3, padding=1, groups=in_channels * expansion_factor),\n",
    "            nn.BatchNorm2d(in_channels * expansion_factor),\n",
    "            nn.GELU(),\n",
    "            SqueezeExcitation(in_channels * expansion_factor, in_channels, activation=nn.GELU),\n",
    "            nn.Conv2d(in_channels * expansion_factor, out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.mb_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsamplingMBConv(MBConv):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor=4):\n",
    "        super().__init__(in_channels, out_channels, expansion_factor=4)\n",
    "        self.mb_conv[1] = nn.Conv2d(in_channels, in_channels * expansion_factor, kernel_size=1, stride = 2)\n",
    "        self.channel_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.channel_projection(self.pool(x)) + self.mb_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeAttention2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, image_size, heads=8, head_size=32):\n",
    "        super().__init__()\n",
    "        heads = out_channels // head_size\n",
    "        self.heads = heads\n",
    "        self.head_size = head_size\n",
    "        self.image_size = image_size\n",
    "        self.head_dim = heads * head_size\n",
    "        self.attend = nn.Softmax(dim=-2) # Taken from My_CoAtNet\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.to_q = nn.Linear(in_channels, self.head_dim)\n",
    "        self.to_k = nn.Linear(in_channels, self.head_dim)\n",
    "        self.to_v = nn.Linear(in_channels, self.head_dim)\n",
    "        self.to_output = nn.Sequential(\n",
    "            nn.Linear(self.head_dim, out_channels),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.normalization = nn.LayerNorm(in_channels)\n",
    "        \n",
    "        self.relative_bias = nn.Parameter(torch.randn(heads, (2 * image_size - 1) * (2 * image_size - 1)))\n",
    "        self.register_buffer(\"relative_indices\", self.get_indices(image_size, image_size)) # Taken from My_CoAtNet\n",
    "        self.precomputed_relative_bias = None\n",
    "    \n",
    "    def norm(self, x):\n",
    "        x = x.transpose(1, -1) # Taken from My_CoAtNet\n",
    "        x = self.normalization(x)\n",
    "        x = x.transpose(-1, 1) # Taken from My_CoAtNet\n",
    "        return x\n",
    "    \n",
    "    def get_relative_biases(self):\n",
    "        # Relative bias caching mentioned in CoAtNet: Marrying Convolution and Attention for All Data Sizes\n",
    "        if not self.training:\n",
    "            return self.precomputed_relative_bias\n",
    "        # Taken from od My_CoAtNet\n",
    "        indices = self.relative_indices.expand(self.heads, -1)\n",
    "        rel_pos_enc = self.relative_bias.gather(-1, indices)\n",
    "        rel_pos_enc = rel_pos_enc.unflatten(-1, (self.image_size * self.image_size, self.image_size * self.image_size))\n",
    "        return rel_pos_enc\n",
    "    \n",
    "    def reshape_for_linear(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        return x.reshape(b, self.image_size * self.image_size, self.in_channels)\n",
    "    \n",
    "    def attention_score(self, x):\n",
    "        b, _, h, _ = x.shape\n",
    "        q = self.to_q(self.reshape_for_linear(x)).view(b, self.heads, self.head_size, -1) # Taken from My_CoAtNet\n",
    "        k = self.to_k(self.reshape_for_linear(x)).view(b, self.heads, self.head_size, -1) # Taken from My_CoAtNet\n",
    "        dots = torch.matmul(k.transpose(-1, -2), q) / math.sqrt(self.head_dim)\n",
    "        relative_biases_indexed = self.get_relative_biases()\n",
    "        return self.attend(dots + relative_biases_indexed)\n",
    "    \n",
    "    def relative_attention(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        v = self.to_v(self.reshape_for_linear(x)).view(b, self.heads, self.head_size, -1) # Taken from My_CoAtNet\n",
    "        out = torch.matmul(v, self.attention_score(x)) # I figured this out after debugging (Still the same as My_CoAtNet)\n",
    "        out = out.view(b, self.image_size, self.image_size, -1)\n",
    "        return self.to_output(out).view(b, self.out_channels, self.image_size, self.image_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.relative_attention(self.norm(x))\n",
    "    \n",
    "    def train(self, training):\n",
    "        if not training:\n",
    "            self.precomputed_relative_bias = self.get_relative_biases()\n",
    "        super().train(training)\n",
    "        \n",
    "    # Taken from My_CoAtNet\n",
    "    @staticmethod\n",
    "    def get_indices(h, w):\n",
    "        y = torch.arange(h, dtype=torch.long)\n",
    "        x = torch.arange(w, dtype=torch.long)\n",
    "        \n",
    "        y1, x1, y2, x2 = torch.meshgrid(y, x, y, x)\n",
    "        indices = (y1 - y2 + h - 1) * (2 * w - 1) + x1 - x2 + w - 1\n",
    "        indices = indices.flatten()\n",
    "        \n",
    "        return indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsamplingRelativeAttention2d(RelativeAttention2d):\n",
    "    def __init__(self, in_channels, out_channels, image_size, heads=8, head_size=32):\n",
    "        super().__init__(in_channels, out_channels, image_size, heads=8, head_size=32)\n",
    "        self.channel_projection = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.normalization = nn.LayerNorm(in_channels)\n",
    "\n",
    "\n",
    "    def norm(self, x):\n",
    "        x = x.transpose(1, -1) # Taken from My_CoAtNet\n",
    "        x = self.normalization(x)\n",
    "        x = x.transpose(-1, 1) # Taken from My_CoAtNet\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.channel_projection(self.pool(x)) + self.relative_attention(self.pool(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, out_channels, expansion_factor=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = out_channels * expansion_factor\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(out_channels, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, out_channels),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.normalization = nn.LayerNorm(out_channels)\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    \n",
    "    def norm(self, x):\n",
    "        x = x.transpose(1, -1) # Taken from My_CoAtNet\n",
    "        x = self.normalization(x)\n",
    "        x = x.transpose(-1, 1) # Taken from My_CoAtNet\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        old_shape = x.shape\n",
    "        batch_size = old_shape[0]\n",
    "        return x + torch.reshape(self.ffn(torch.reshape(self.norm(x), (batch_size, -1, self.out_channels))), old_shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleTransformerBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, image_size):\n",
    "        attention = DownsamplingRelativeAttention2d(in_channels, out_channels, image_size)\n",
    "        ffn = FeedForwardNetwork(out_channels)\n",
    "        super().__init__(\n",
    "            attention,\n",
    "            ffn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, image_size):\n",
    "        attention = RelativeAttention2d(in_channels, out_channels, image_size)\n",
    "        ffn = FeedForwardNetwork(out_channels)\n",
    "        super().__init__(\n",
    "            attention,\n",
    "            ffn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(in_channels, num_classes)\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.pool(x)\n",
    "        x = torch.reshape(x, (batch_size, -1, self.in_channels))\n",
    "        return torch.squeeze(self.fc(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCoAtNet(nn.Sequential):\n",
    "    def __init__(self, nums_blocks, layer_out_channels, num_classes=36):\n",
    "        s0 = nn.Sequential(Stem(layer_out_channels[0]))\n",
    "        \n",
    "        s1 = [DownsamplingMBConv(layer_out_channels[0], layer_out_channels[1])]\n",
    "        for i in range(nums_blocks[1] - 1):\n",
    "            s1.append(MBConv(layer_out_channels[1], layer_out_channels[1]))\n",
    "        s1 = nn.Sequential(*s1)\n",
    "        \n",
    "        s2 = [DownsamplingMBConv(layer_out_channels[1], layer_out_channels[2])]\n",
    "        for i in range(nums_blocks[2] - 1):\n",
    "            s2.append(MBConv(layer_out_channels[2], layer_out_channels[2]))\n",
    "        s2 = nn.Sequential(*s2)\n",
    "        \n",
    "        s3 = [DownsampleTransformerBlock(layer_out_channels[2], layer_out_channels[3], 64 // 16)]\n",
    "        for i in range(nums_blocks[3] - 1):\n",
    "            s3.append(TransformerBlock(layer_out_channels[3], layer_out_channels[3], 64 // 16))\n",
    "        s3 = nn.Sequential(*s3)\n",
    "        \n",
    "        s4 = [DownsampleTransformerBlock(layer_out_channels[3], layer_out_channels[4], 64 // 32)]\n",
    "        for i in range(nums_blocks[4] - 1):\n",
    "            s4.append(TransformerBlock(layer_out_channels[4], layer_out_channels[4], 64 // 32))\n",
    "        s4 = nn.Sequential(*s4)\n",
    "        \n",
    "        head = Head(layer_out_channels[4], num_classes)\n",
    "        \n",
    "        super().__init__(\n",
    "            s0,\n",
    "            s1,\n",
    "            s2,\n",
    "            s3,\n",
    "            s4,\n",
    "            head\n",
    "        )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/venv/lib/python3.10/site-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# CoAtNet-1\n",
    "nums_blocks = [2, 2, 3, 5, 2]           # L\n",
    "channels = [64, 96, 192, 384, 768]      # D\n",
    "\n",
    "model = MyCoAtNet(nums_blocks, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 24,033,296\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]             640\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              GELU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4           [-1, 64, 30, 30]          36,928\n",
      "         MaxPool2d-5           [-1, 64, 15, 15]               0\n",
      "            Conv2d-6           [-1, 96, 15, 15]           6,144\n",
      "       BatchNorm2d-7           [-1, 64, 30, 30]             128\n",
      "            Conv2d-8          [-1, 256, 15, 15]          16,640\n",
      "       BatchNorm2d-9          [-1, 256, 15, 15]             512\n",
      "             GELU-10          [-1, 256, 15, 15]               0\n",
      "           Conv2d-11          [-1, 256, 15, 15]           2,560\n",
      "      BatchNorm2d-12          [-1, 256, 15, 15]             512\n",
      "             GELU-13          [-1, 256, 15, 15]               0\n",
      "AdaptiveAvgPool2d-14            [-1, 256, 1, 1]               0\n",
      "           Conv2d-15             [-1, 64, 1, 1]          16,448\n",
      "             GELU-16             [-1, 64, 1, 1]               0\n",
      "           Conv2d-17            [-1, 256, 1, 1]          16,640\n",
      "          Sigmoid-18            [-1, 256, 1, 1]               0\n",
      "SqueezeExcitation-19          [-1, 256, 15, 15]               0\n",
      "           Conv2d-20           [-1, 96, 15, 15]          24,672\n",
      "      BatchNorm2d-21           [-1, 96, 15, 15]             192\n",
      "DownsamplingMBConv-22           [-1, 96, 15, 15]               0\n",
      "      BatchNorm2d-23           [-1, 96, 15, 15]             192\n",
      "           Conv2d-24          [-1, 384, 15, 15]          37,248\n",
      "      BatchNorm2d-25          [-1, 384, 15, 15]             768\n",
      "             GELU-26          [-1, 384, 15, 15]               0\n",
      "           Conv2d-27          [-1, 384, 15, 15]           3,840\n",
      "      BatchNorm2d-28          [-1, 384, 15, 15]             768\n",
      "             GELU-29          [-1, 384, 15, 15]               0\n",
      "AdaptiveAvgPool2d-30            [-1, 384, 1, 1]               0\n",
      "           Conv2d-31             [-1, 96, 1, 1]          36,960\n",
      "             GELU-32             [-1, 96, 1, 1]               0\n",
      "           Conv2d-33            [-1, 384, 1, 1]          37,248\n",
      "          Sigmoid-34            [-1, 384, 1, 1]               0\n",
      "SqueezeExcitation-35          [-1, 384, 15, 15]               0\n",
      "           Conv2d-36           [-1, 96, 15, 15]          36,960\n",
      "      BatchNorm2d-37           [-1, 96, 15, 15]             192\n",
      "           MBConv-38           [-1, 96, 15, 15]               0\n",
      "        MaxPool2d-39             [-1, 96, 8, 8]               0\n",
      "           Conv2d-40            [-1, 192, 8, 8]          18,432\n",
      "      BatchNorm2d-41           [-1, 96, 15, 15]             192\n",
      "           Conv2d-42            [-1, 384, 8, 8]          37,248\n",
      "      BatchNorm2d-43            [-1, 384, 8, 8]             768\n",
      "             GELU-44            [-1, 384, 8, 8]               0\n",
      "           Conv2d-45            [-1, 384, 8, 8]           3,840\n",
      "      BatchNorm2d-46            [-1, 384, 8, 8]             768\n",
      "             GELU-47            [-1, 384, 8, 8]               0\n",
      "AdaptiveAvgPool2d-48            [-1, 384, 1, 1]               0\n",
      "           Conv2d-49             [-1, 96, 1, 1]          36,960\n",
      "             GELU-50             [-1, 96, 1, 1]               0\n",
      "           Conv2d-51            [-1, 384, 1, 1]          37,248\n",
      "          Sigmoid-52            [-1, 384, 1, 1]               0\n",
      "SqueezeExcitation-53            [-1, 384, 8, 8]               0\n",
      "           Conv2d-54            [-1, 192, 8, 8]          73,920\n",
      "      BatchNorm2d-55            [-1, 192, 8, 8]             384\n",
      "DownsamplingMBConv-56            [-1, 192, 8, 8]               0\n",
      "      BatchNorm2d-57            [-1, 192, 8, 8]             384\n",
      "           Conv2d-58            [-1, 768, 8, 8]         148,224\n",
      "      BatchNorm2d-59            [-1, 768, 8, 8]           1,536\n",
      "             GELU-60            [-1, 768, 8, 8]               0\n",
      "           Conv2d-61            [-1, 768, 8, 8]           7,680\n",
      "      BatchNorm2d-62            [-1, 768, 8, 8]           1,536\n",
      "             GELU-63            [-1, 768, 8, 8]               0\n",
      "AdaptiveAvgPool2d-64            [-1, 768, 1, 1]               0\n",
      "           Conv2d-65            [-1, 192, 1, 1]         147,648\n",
      "             GELU-66            [-1, 192, 1, 1]               0\n",
      "           Conv2d-67            [-1, 768, 1, 1]         148,224\n",
      "          Sigmoid-68            [-1, 768, 1, 1]               0\n",
      "SqueezeExcitation-69            [-1, 768, 8, 8]               0\n",
      "           Conv2d-70            [-1, 192, 8, 8]         147,648\n",
      "      BatchNorm2d-71            [-1, 192, 8, 8]             384\n",
      "           MBConv-72            [-1, 192, 8, 8]               0\n",
      "      BatchNorm2d-73            [-1, 192, 8, 8]             384\n",
      "           Conv2d-74            [-1, 768, 8, 8]         148,224\n",
      "      BatchNorm2d-75            [-1, 768, 8, 8]           1,536\n",
      "             GELU-76            [-1, 768, 8, 8]               0\n",
      "           Conv2d-77            [-1, 768, 8, 8]           7,680\n",
      "      BatchNorm2d-78            [-1, 768, 8, 8]           1,536\n",
      "             GELU-79            [-1, 768, 8, 8]               0\n",
      "AdaptiveAvgPool2d-80            [-1, 768, 1, 1]               0\n",
      "           Conv2d-81            [-1, 192, 1, 1]         147,648\n",
      "             GELU-82            [-1, 192, 1, 1]               0\n",
      "           Conv2d-83            [-1, 768, 1, 1]         148,224\n",
      "          Sigmoid-84            [-1, 768, 1, 1]               0\n",
      "SqueezeExcitation-85            [-1, 768, 8, 8]               0\n",
      "           Conv2d-86            [-1, 192, 8, 8]         147,648\n",
      "      BatchNorm2d-87            [-1, 192, 8, 8]             384\n",
      "           MBConv-88            [-1, 192, 8, 8]               0\n",
      "        MaxPool2d-89            [-1, 192, 4, 4]               0\n",
      "           Conv2d-90            [-1, 384, 4, 4]          73,728\n",
      "        LayerNorm-91            [-1, 8, 8, 192]             384\n",
      "        MaxPool2d-92            [-1, 192, 4, 4]               0\n",
      "           Linear-93              [-1, 16, 384]          74,112\n",
      "           Linear-94              [-1, 16, 384]          74,112\n",
      "           Linear-95              [-1, 16, 384]          74,112\n",
      "          Softmax-96           [-1, 12, 16, 16]               0\n",
      "           Linear-97            [-1, 4, 4, 384]         147,840\n",
      "          Dropout-98            [-1, 4, 4, 384]               0\n",
      "DownsamplingRelativeAttention2d-99            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-100            [-1, 4, 4, 384]             768\n",
      "          Linear-101             [-1, 16, 1536]         591,360\n",
      "            GELU-102             [-1, 16, 1536]               0\n",
      "         Dropout-103             [-1, 16, 1536]               0\n",
      "          Linear-104              [-1, 16, 384]         590,208\n",
      "         Dropout-105              [-1, 16, 384]               0\n",
      "FeedForwardNetwork-106            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-107            [-1, 4, 4, 384]             768\n",
      "          Linear-108              [-1, 16, 384]         147,840\n",
      "          Linear-109              [-1, 16, 384]         147,840\n",
      "          Linear-110              [-1, 16, 384]         147,840\n",
      "         Softmax-111           [-1, 12, 16, 16]               0\n",
      "          Linear-112            [-1, 4, 4, 384]         147,840\n",
      "         Dropout-113            [-1, 4, 4, 384]               0\n",
      "RelativeAttention2d-114            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-115            [-1, 4, 4, 384]             768\n",
      "          Linear-116             [-1, 16, 1536]         591,360\n",
      "            GELU-117             [-1, 16, 1536]               0\n",
      "         Dropout-118             [-1, 16, 1536]               0\n",
      "          Linear-119              [-1, 16, 384]         590,208\n",
      "         Dropout-120              [-1, 16, 384]               0\n",
      "FeedForwardNetwork-121            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-122            [-1, 4, 4, 384]             768\n",
      "          Linear-123              [-1, 16, 384]         147,840\n",
      "          Linear-124              [-1, 16, 384]         147,840\n",
      "          Linear-125              [-1, 16, 384]         147,840\n",
      "         Softmax-126           [-1, 12, 16, 16]               0\n",
      "          Linear-127            [-1, 4, 4, 384]         147,840\n",
      "         Dropout-128            [-1, 4, 4, 384]               0\n",
      "RelativeAttention2d-129            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-130            [-1, 4, 4, 384]             768\n",
      "          Linear-131             [-1, 16, 1536]         591,360\n",
      "            GELU-132             [-1, 16, 1536]               0\n",
      "         Dropout-133             [-1, 16, 1536]               0\n",
      "          Linear-134              [-1, 16, 384]         590,208\n",
      "         Dropout-135              [-1, 16, 384]               0\n",
      "FeedForwardNetwork-136            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-137            [-1, 4, 4, 384]             768\n",
      "          Linear-138              [-1, 16, 384]         147,840\n",
      "          Linear-139              [-1, 16, 384]         147,840\n",
      "          Linear-140              [-1, 16, 384]         147,840\n",
      "         Softmax-141           [-1, 12, 16, 16]               0\n",
      "          Linear-142            [-1, 4, 4, 384]         147,840\n",
      "         Dropout-143            [-1, 4, 4, 384]               0\n",
      "RelativeAttention2d-144            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-145            [-1, 4, 4, 384]             768\n",
      "          Linear-146             [-1, 16, 1536]         591,360\n",
      "            GELU-147             [-1, 16, 1536]               0\n",
      "         Dropout-148             [-1, 16, 1536]               0\n",
      "          Linear-149              [-1, 16, 384]         590,208\n",
      "         Dropout-150              [-1, 16, 384]               0\n",
      "FeedForwardNetwork-151            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-152            [-1, 4, 4, 384]             768\n",
      "          Linear-153              [-1, 16, 384]         147,840\n",
      "          Linear-154              [-1, 16, 384]         147,840\n",
      "          Linear-155              [-1, 16, 384]         147,840\n",
      "         Softmax-156           [-1, 12, 16, 16]               0\n",
      "          Linear-157            [-1, 4, 4, 384]         147,840\n",
      "         Dropout-158            [-1, 4, 4, 384]               0\n",
      "RelativeAttention2d-159            [-1, 384, 4, 4]               0\n",
      "       LayerNorm-160            [-1, 4, 4, 384]             768\n",
      "          Linear-161             [-1, 16, 1536]         591,360\n",
      "            GELU-162             [-1, 16, 1536]               0\n",
      "         Dropout-163             [-1, 16, 1536]               0\n",
      "          Linear-164              [-1, 16, 384]         590,208\n",
      "         Dropout-165              [-1, 16, 384]               0\n",
      "FeedForwardNetwork-166            [-1, 384, 4, 4]               0\n",
      "       MaxPool2d-167            [-1, 384, 2, 2]               0\n",
      "          Conv2d-168            [-1, 768, 2, 2]         294,912\n",
      "       LayerNorm-169            [-1, 4, 4, 384]             768\n",
      "       MaxPool2d-170            [-1, 384, 2, 2]               0\n",
      "          Linear-171               [-1, 4, 768]         295,680\n",
      "          Linear-172               [-1, 4, 768]         295,680\n",
      "          Linear-173               [-1, 4, 768]         295,680\n",
      "         Softmax-174             [-1, 24, 4, 4]               0\n",
      "          Linear-175            [-1, 2, 2, 768]         590,592\n",
      "         Dropout-176            [-1, 2, 2, 768]               0\n",
      "DownsamplingRelativeAttention2d-177            [-1, 768, 2, 2]               0\n",
      "       LayerNorm-178            [-1, 2, 2, 768]           1,536\n",
      "          Linear-179              [-1, 4, 3072]       2,362,368\n",
      "            GELU-180              [-1, 4, 3072]               0\n",
      "         Dropout-181              [-1, 4, 3072]               0\n",
      "          Linear-182               [-1, 4, 768]       2,360,064\n",
      "         Dropout-183               [-1, 4, 768]               0\n",
      "FeedForwardNetwork-184            [-1, 768, 2, 2]               0\n",
      "       LayerNorm-185            [-1, 2, 2, 768]           1,536\n",
      "          Linear-186               [-1, 4, 768]         590,592\n",
      "          Linear-187               [-1, 4, 768]         590,592\n",
      "          Linear-188               [-1, 4, 768]         590,592\n",
      "         Softmax-189             [-1, 24, 4, 4]               0\n",
      "          Linear-190            [-1, 2, 2, 768]         590,592\n",
      "         Dropout-191            [-1, 2, 2, 768]               0\n",
      "RelativeAttention2d-192            [-1, 768, 2, 2]               0\n",
      "       LayerNorm-193            [-1, 2, 2, 768]           1,536\n",
      "          Linear-194              [-1, 4, 3072]       2,362,368\n",
      "            GELU-195              [-1, 4, 3072]               0\n",
      "         Dropout-196              [-1, 4, 3072]               0\n",
      "          Linear-197               [-1, 4, 768]       2,360,064\n",
      "         Dropout-198               [-1, 4, 768]               0\n",
      "FeedForwardNetwork-199            [-1, 768, 2, 2]               0\n",
      "AdaptiveAvgPool2d-200            [-1, 768, 1, 1]               0\n",
      "          Linear-201                [-1, 1, 36]          27,684\n",
      "            Head-202                   [-1, 36]               0\n",
      "================================================================\n",
      "Total params: 24,029,924\n",
      "Trainable params: 24,029,924\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 26.27\n",
      "Params size (MB): 91.67\n",
      "Estimated Total Size (MB): 117.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.apply(init_linear)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))\n",
    "\n",
    "summary(model, input_size=(1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCoAtNet(\n",
       "  (0): Sequential(\n",
       "    (0): Stem(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): DownsamplingMBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (channel_projection): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "        (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): DownsamplingMBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(96, 384, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "        (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (channel_projection): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        (5): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): MBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        (5): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): DownsampleTransformerBlock(\n",
       "      (0): DownsamplingRelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=192, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=192, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=192, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (channel_projection): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): DownsampleTransformerBlock(\n",
       "      (0): DownsamplingRelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (channel_projection): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): Head(\n",
       "    (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (fc): Linear(in_features=768, out_features=36, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.load_state_dict(torch.load(\"CoAtNet-1-Best-Zoom.pkl\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from My_CoAtNet\n",
    "def separate_parameters(model):\n",
    "    parameters_decay = set()\n",
    "    parameters_no_decay = set()\n",
    "    modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
    "    modules_no_weight_decay = (nn.LayerNorm, nn.BatchNorm2d)\n",
    "    \n",
    "    for m_name, m in model.named_modules():\n",
    "        for param_name, param in m.named_parameters():\n",
    "            full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
    "\n",
    "            if isinstance(m, modules_no_weight_decay):\n",
    "                parameters_no_decay.add(full_param_name)\n",
    "            elif param_name.endswith(\"bias\"):\n",
    "                parameters_no_decay.add(full_param_name)\n",
    "            elif isinstance(m, modules_weight_decay):\n",
    "                parameters_decay.add(full_param_name)\n",
    "    \n",
    "    # sanity check\n",
    "    assert len(parameters_decay & parameters_no_decay) == 0\n",
    "    assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
    "\n",
    "    return parameters_decay, parameters_no_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.1\n",
    "num_epochs = 1100\n",
    "min_learning_rate = 1e-6\n",
    "\n",
    "param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "parameters_decay, parameters_no_decay = separate_parameters(model)\n",
    "parameter_groups = [\n",
    "    {\"params\": [param_dict[pn] for pn in parameters_decay], \"weight_decay\": weight_decay},\n",
    "    {\"params\": [param_dict[pn] for pn in parameters_no_decay], \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.Adam(parameter_groups, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=min_learning_rate / learning_rate, total_iters=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from My_CoAtNet\n",
    "def plot_results(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"Training loss\", \"Validation loss\"], loc =  \"best\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.plot(train_accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"Training accuracy\", \"Validation accuracy\"], loc =  \"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"CoAtNet-1-Best-Zoom.pkl\"\n",
    "model_path = \"CoAtNet-1-Zoom.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCoAtNet(\n",
       "  (0): Sequential(\n",
       "    (0): Stem(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): DownsamplingMBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "        (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (channel_projection): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "        (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): DownsamplingMBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(96, 384, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "        (5): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (channel_projection): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): MBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        (5): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): MBConv(\n",
       "      (mb_conv): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "        (5): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): GELU(approximate='none')\n",
       "          (scale_activation): Sigmoid()\n",
       "        )\n",
       "        (8): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (9): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): DownsampleTransformerBlock(\n",
       "      (0): DownsamplingRelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=192, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=192, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=192, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (channel_projection): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): DownsampleTransformerBlock(\n",
       "      (0): DownsamplingRelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (to_k): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (to_v): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (channel_projection): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (0): RelativeAttention2d(\n",
       "        (attend): Softmax(dim=-2)\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (to_output): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): FeedForwardNetwork(\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.3, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (normalization): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): Head(\n",
       "    (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (fc): Linear(in_features=768, out_features=36, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings from indices to syllables and vice versa\n",
    "digits_and_syllables = list('0123456789abcdefghijklmnopqrstuvwxyz')\n",
    "idx_to_syllable = {idx: syllable for idx, syllable in enumerate(digits_and_syllables)}\n",
    "syllable_to_idx = {syllable: idx for idx, syllable in idx_to_syllable.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine test and validation datasets\n",
    "combined_dataset = ConcatDataset([val_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from syllables to dataset indices\n",
    "syllable_to_indices = {}\n",
    "for idx in range(len(combined_dataset)):\n",
    "    _, label = combined_dataset[idx]\n",
    "    # label = label.item() \n",
    "    syllable = idx_to_syllable[label]\n",
    "    if syllable not in syllable_to_indices:\n",
    "        syllable_to_indices[syllable] = []\n",
    "    syllable_to_indices[syllable].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process sentences into syllables\n",
    "def get_syllables(sentence):\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    # sentence = sentence.replace(' ',  '').lower()\n",
    "    syllables = list(sentence)\n",
    "    return syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 12308\n"
     ]
    }
   ],
   "source": [
    "# Read sentences from the text file\n",
    "with open('../sentences/ft_sentences.txt', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "print(\"Number of sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def compute_accuracy_and_wrong_syllables(true_sentence, predicted_sentence):\n",
    "    # Character-level accuracy using SequenceMatcher\n",
    "    char_matcher = difflib.SequenceMatcher(None, true_sentence, predicted_sentence)\n",
    "    accuracy = char_matcher.ratio()\n",
    "    \n",
    "    # Word-level wrong syllable count using SequenceMatcher on word lists\n",
    "    true_words = true_sentence.split()\n",
    "    predicted_words = predicted_sentence.split()\n",
    "    word_matcher = difflib.SequenceMatcher(None, true_words, predicted_words)\n",
    "    \n",
    "    # Calculate wrong syllables based on insert, delete, and replace operations\n",
    "    wrong_syllables = sum(1 for tag, _, _, _, _ in word_matcher.get_opcodes() if tag in ('insert', 'delete', 'replace'))\n",
    "    \n",
    "    return accuracy, wrong_syllables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process each sentence\n",
    "# noise_factor = 0\n",
    "# results = []\n",
    "\n",
    "# for sentence in tqdm(sentences[:]):\n",
    "#     sentence = sentence.strip()\n",
    "#     syllables = get_syllables(sentence)\n",
    "#     true_sentence = ''.join(syllables)\n",
    "#     predicted_syllables = []\n",
    "\n",
    "#     for syllable in syllables:\n",
    "#         if syllable in syllable_to_indices:\n",
    "#             # Randomly select an index for the syllable\n",
    "#             idx = random.choice(syllable_to_indices[syllable])\n",
    "#             # Retrieve the image and label from the dataset\n",
    "#             image, _ = combined_dataset[idx]\n",
    "\n",
    "#             noise = torch.randn_like(image) * noise_factor\n",
    "#             image = image + noise\n",
    "#             image = image.unsqueeze(0).to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 output = model(image)\n",
    "#                 output = output.reshape(1, -1)\n",
    "#                 _, predicted_idx = torch.max(output.data, 1)\n",
    "#                 predicted_syllable = idx_to_syllable[predicted_idx.item()]\n",
    "#         else:\n",
    "#             if random.random() < 0.90:\n",
    "#                 predicted_syllable = ' '\n",
    "#             else:\n",
    "#                 predicted_syllable = random.choice(digits_and_syllables)\n",
    "#         predicted_syllables.append(predicted_syllable)\n",
    "    \n",
    "#     predicted_sentence = ''.join(predicted_syllables)\n",
    "#     accuracy, wrong_syllables = compute_accuracy_and_wrong_syllables(true_sentence, predicted_sentence)\n",
    "#     results.append([true_sentence, predicted_sentence, accuracy, wrong_syllables])\n",
    "#     # print(f\"accuracy: {accuracy}, wrong: {wrong_syllables}, true: {true_sentence}, predicted: {predicted_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [39:25<00:00,  3.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process sentences in batches\n",
    "batch_size = 17  # Adjust based on available GPU memory\n",
    "noise_factor = 0\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(0, len(sentences[:]), batch_size)):\n",
    "    batch_sentences = sentences[i : i + batch_size]\n",
    "    batch_sentences = [sentence.strip() for sentence in batch_sentences]\n",
    "    \n",
    "    batch_true_sentences = [''.join(get_syllables(sentence)) for sentence in batch_sentences]\n",
    "    batch_predicted_sentences = []\n",
    "\n",
    "    for true_sentence in batch_true_sentences:\n",
    "        syllables = list(true_sentence)\n",
    "        batch_images = []\n",
    "        batch_valid_indices = []\n",
    "\n",
    "        # Collect syllables in a batch\n",
    "        for syllable in syllables:\n",
    "            if syllable in syllable_to_indices:\n",
    "                idx = random.choice(syllable_to_indices[syllable])\n",
    "                image, _ = combined_dataset[idx]\n",
    "                noise = torch.randn_like(image) * noise_factor\n",
    "                batch_images.append(image + noise)\n",
    "                batch_valid_indices.append(len(batch_images) - 1)\n",
    "            else:\n",
    "                batch_images.append(None)\n",
    "                batch_valid_indices.append(None)\n",
    "\n",
    "        # Process valid images as a batch\n",
    "        if any(img is not None for img in batch_images):\n",
    "            valid_images = [img for img in batch_images if img is not None]\n",
    "            valid_images = torch.stack(valid_images).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(valid_images)\n",
    "                _, predicted_indices = torch.max(output.data, 1)\n",
    "\n",
    "            predicted_syllables = [''] * len(batch_images)\n",
    "            valid_idx = 0\n",
    "            for j, idx in enumerate(batch_valid_indices):\n",
    "                if idx is not None:\n",
    "                    predicted_syllables[j] = idx_to_syllable[predicted_indices[valid_idx].item()]\n",
    "                    valid_idx += 1\n",
    "                else:\n",
    "                    predicted_syllables[j] = ' ' if random.random() < 0.90 else random.choice(digits_and_syllables)\n",
    "\n",
    "            predicted_sentence = ''.join(predicted_syllables)\n",
    "        else:\n",
    "            print(\"oh no\")\n",
    "            predicted_sentence = ' ' * len(true_sentence)\n",
    "\n",
    "        batch_predicted_sentences.append(predicted_sentence)\n",
    "\n",
    "    # Compute accuracy and wrong syllables for the batch\n",
    "    for true_sentence, predicted_sentence in zip(batch_true_sentences, batch_predicted_sentences):\n",
    "        accuracy, wrong_syllables = compute_accuracy_and_wrong_syllables(true_sentence, predicted_sentence)\n",
    "        results.append([true_sentence, predicted_sentence, accuracy, wrong_syllables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12308"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise factor: 0\n",
      "Average accuracy: 0.6434\n",
      "Total wrong syllables: 23835\n"
     ]
    }
   ],
   "source": [
    "# accuracy average and wrong syllables sum\n",
    "accuracy_avg = sum(result[2] for result in results) / len(results)\n",
    "wrong_syllables_sum = sum(result[3] for result in results)\n",
    "\n",
    "print(\"Noise factor:\", noise_factor)\n",
    "print(f\"Average accuracy: {accuracy_avg:.4f}\")\n",
    "print(f\"Total wrong syllables: {wrong_syllables_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/ft_noise_1.0.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['True Sentence', 'Predicted Sentence', 'Accuracy', 'Wrong syllables'])\n",
    "    for row in results:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files combined successfully.\n"
     ]
    }
   ],
   "source": [
    "# concatenate all the results\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "csv_files = glob.glob(\"results/ft_noise_*.csv\")\n",
    "df_list = []\n",
    "\n",
    "for file in csv_files:\n",
    "    noise_factor = float(file.split('_')[-1].replace('.csv', ''))\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"Noise Factor\"] = noise_factor\n",
    "    df_list.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "# shuffle the dataframe\n",
    "final_df = final_df.sample(frac=1).reset_index(drop=True)\n",
    "# Save to a new CSV file\n",
    "final_df.to_csv(\"results/ft_zoom_ds.csv\", index=False)\n",
    "\n",
    "print(\"CSV files combined successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
