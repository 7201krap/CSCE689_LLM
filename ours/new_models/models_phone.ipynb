{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification, SwinForImageClassification, Swinv2ForImageClassification, DeiTForImageClassification, BeitForImageClassification\n",
    "from transformers import AutoImageProcessor\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "dataset_dir = '../../img_dataset_phone'\n",
    "\n",
    "# select models\n",
    "vit     = False\n",
    "swin    = False \n",
    "swin2   = False\n",
    "deit    = False\n",
    "beit    = False\n",
    "\n",
    "# define models\n",
    "if swin:\n",
    "    model_name = 'microsoft/swin-tiny-patch4-window7-224'\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = SwinForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=36,\n",
    "        ignore_mismatched_sizes=True  \n",
    "    )\n",
    "elif swin2:\n",
    "    model_name = 'microsoft/swinv2-tiny-patch4-window16-256'\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = Swinv2ForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=36,\n",
    "        ignore_mismatched_sizes=True  \n",
    "    )\n",
    "elif vit:\n",
    "    model_name = 'google/vit-base-patch16-224-in21k'\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=36,\n",
    "        # ignore_mismatched_sizes=True,\n",
    "    )\n",
    "elif deit:\n",
    "    model_name = 'facebook/deit-base-distilled-patch16-224'\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = DeiTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=36,\n",
    "        # ignore_mismatched_sizes=True\n",
    "    )\n",
    "elif beit:\n",
    "    model_name = 'microsoft/beit-base-patch16-224-pt22k-ft22k'  \n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = BeitForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=36,\n",
    "        ignore_mismatched_sizes=True  # Add this if necessary\n",
    "    )\n",
    "else:\n",
    "    raise ValueError('[ERROR] Select Your Model')\n",
    "\n",
    "# Define transformations\n",
    "if vit or swin or deit or beit:\n",
    "    print(\"vit/swin/deit/beit activated\")\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "elif swin2:\n",
    "    print(\"swin2 activated\")\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "else:\n",
    "    raise ValueError('[ERROR] Define any transformations')\n",
    "\n",
    "# Load the dataset\n",
    "full_dataset = ImageFolder(root=dataset_dir, transform=train_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset per class into train and test sets\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get the targets for all samples\n",
    "targets = np.array([sample[1] for sample in full_dataset.samples])\n",
    "\n",
    "# Create a dictionary mapping each class to the indices of its samples\n",
    "class_indices = defaultdict(list)\n",
    "for idx, target in enumerate(targets):\n",
    "    class_indices[target].append(idx)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# For each class, split the indices into train and test\n",
    "for cls, indices in class_indices.items():\n",
    "    np.random.shuffle(indices)\n",
    "    n_train = int(0.8 * len(indices))  # 80% for training\n",
    "    train_indices.extend(indices[:n_train])\n",
    "    test_indices.extend(indices[n_train:])\n",
    "\n",
    "# Create Subset datasets\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(full_dataset, test_indices)\n",
    "\n",
    "# Apply test transforms to test dataset\n",
    "test_dataset.dataset.transform = test_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_accuracy = 0.0\n",
    "patience = 25  # Number of epochs to wait before early stopping\n",
    "epochs_no_improve = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss / total\n",
    "    train_accuracy = correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Validation\", leave=False):\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / total\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_accuracy)\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), f'best_model_{model_name}.pth')\n",
    "        print(\"Validation accuracy improved, model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation accuracy for {epochs_no_improve} epochs.\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model and generating a classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the best model \n",
    "model.load_state_dict(torch.load(f'best_model_{model_name}.pth'))\n",
    "\n",
    "# Collect all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(all_labels, all_preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
